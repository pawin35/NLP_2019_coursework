{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Dictionary-based Tokenization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n",
    "<br>\n",
    "* maximal_matching\n",
    "* backtrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy dictionary to test the algorithm\n",
    "\n",
    "This is based on the example shown in the lecture. \n",
    "You will tokenize the following text string: \"ไปหามเหสี!\"\n",
    "The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal matching \n",
    "Complete the maximal matching  function below to tokenize the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf #infinity\n",
    "def maximal_matching(c):\n",
    "\t#Initialize an empty 2D list\n",
    "\td  =[[None]*len(c) for _ in range(len(c))]\n",
    "\t\n",
    "\tfor i in range(len(c)):\n",
    "\t\tfor j in range(i, len(c)):\n",
    "\t\t\tconsideredWord = c[i:j+1]\n",
    "\t\t\tif consideredWord in thai_vocab:\n",
    "\t\t\t\tval = 1\n",
    "\t\t\t\tmin = inf\n",
    "\t\t\t\tfor k in range(i):\n",
    "\t\t\t\t\tif d[k][i-1] < min:\n",
    "\t\t\t\t\t\tmin = d[k][i-1]\n",
    "\t\t\t\tmin = min if min != inf else 0\n",
    "\t\t\t\td[i][j] = val + min\n",
    "\t\t\telse:\n",
    "\t\t\t\td[i][j] = inf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "\t######################\n",
    "\t\n",
    "\treturn d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtracking\n",
    "Complete the backtracking function below to find the tokenzied words.\n",
    "It should return a list containing a pair of the beginning position and the ending position of each word.\n",
    "In this example, it should return: \n",
    "<br>\n",
    "[(0, 1),(2, 3),(4, 8),(9, 9)]\n",
    "<br> \n",
    "#### Each pair contains the position of each word as follows:\n",
    "(0, 1) ไป\n",
    "<br>\n",
    "(2, 3) หา\n",
    "<br>\n",
    "(4, 8) มเหสี\n",
    "<br>\n",
    "(9, 9) !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(d):\n",
    "    eow = len(d)-1 # End of Word position\n",
    "    word_pos = [] # Word position\n",
    "    ####FILL CODE HERE####\n",
    "    \n",
    "    ######################\n",
    "    word_pos.reverse()\n",
    "    return word_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your maximal matching algorithm on a toy dictionary\n",
    "\n",
    "Expected output:\n",
    "\n",
    "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
    "<br>\n",
    "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, None, 4] !\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
      "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
      "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
      "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
      "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
      "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
      "[None, None, None, None, None, None, None, None, None, 4] !\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี!\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your backtracking algorithm on a toy dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "ไป|หา|มเหสี|!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenized_text(d, input_text):\n",
    "    tokenized_text=[]\n",
    "    for pos in backtrack(d):\n",
    "        #print(pos)\n",
    "        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n",
    "\n",
    "    print(\"|\".join(tokenized_text))\n",
    "    \n",
    "print_tokenized_text(out,input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it on a real dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n",
    "    thai_vocab = f.read().splitlines() \n",
    "print(\"Vocab size:\", len(thai_vocab))\n",
    "print(thai_vocab[:10])\n",
    "#you can add more vocab to the dictionary \n",
    "thai_vocab.extend([\"ๆ\",\"!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output of your maximal matching algoithm on a new dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 2, inf, inf, inf, 2] ม\n",
    "<br>\n",
    "[None, None, None, None, None, inf, 3, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, None] ี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"ไปหามเหสี\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected tokenized text\n",
    "ไปหา|มเหสี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokenized_text(out,input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
