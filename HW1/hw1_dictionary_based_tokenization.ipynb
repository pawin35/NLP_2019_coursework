{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Dictionary-based Tokenization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n",
    "<br>\n",
    "* maximal_matching\n",
    "* backtrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy dictionary to test the algorithm\n",
    "\n",
    "This is based on the example shown in the lecture. \n",
    "You will tokenize the following text string: \"ไปหามเหสี!\"\n",
    "The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal matching \n",
    "Complete the maximal matching  function below to tokenize the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf #infinity\n",
    "def maximal_matching(c):\n",
    "\t#Initialize an empty 2D list\n",
    "\td  =[[None]*len(c) for _ in range(len(c))]\n",
    "\t\n",
    "\tfor i in range(len(c)):\n",
    "\t\tfor j in range(i, len(c)):\n",
    "\t\t\tconsideredWord = c[i:j+1]\n",
    "\t\t\tif consideredWord in thai_vocab or i == j:\n",
    "\t\t\t\td[i][j] = 1\n",
    "\t\t\t\tmin = inf\n",
    "\t\t\t\tfor k in range(i):\n",
    "\t\t\t\t\tif d[k][i-1] < min:\n",
    "\t\t\t\t\t\tmin = d[k][i-1]\n",
    "\t\t\t\tif min != inf:\n",
    "\t\t\t\t\td[i][j] = d[i][j] + min\n",
    "\t\t\telse:\n",
    "\t\t\t\td[i][j] = inf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "\t######################\n",
    "\t\n",
    "\treturn d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtracking\n",
    "Complete the backtracking function below to find the tokenzied words.\n",
    "It should return a list containing a pair of the beginning position and the ending position of each word.\n",
    "In this example, it should return: \n",
    "<br>\n",
    "[(0, 1),(2, 3),(4, 8),(9, 9)]\n",
    "<br> \n",
    "#### Each pair contains the position of each word as follows:\n",
    "(0, 1) ไป\n",
    "<br>\n",
    "(2, 3) หา\n",
    "<br>\n",
    "(4, 8) มเหสี\n",
    "<br>\n",
    "(9, 9) !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(d):\n",
    "\teow = len(d)-1 # End of Word position\n",
    "\tword_pos = [] # Word position\n",
    "\n",
    "\tcurrentEnd = eow\n",
    "\tcurrentStart = eow\n",
    "\t#print((currentStart, currentEnd))\n",
    "\twhile(currentEnd >= 0):\n",
    "\t\t#print((currentStart, currentEnd))\n",
    "\t\tcurrentMin = d[currentEnd][currentEnd]\n",
    "\t\tfor j in range(currentEnd, -1, -1):\n",
    "\t\t\tif d[j][currentEnd] < currentMin:\n",
    "\t\t\t\tcurrentMin = d[j][currentEnd]\n",
    "\t\t\t\tcurrentStart = j\n",
    "\t\tword_pos.append((currentStart, currentEnd))\n",
    "\t\tcurrentEnd = currentStart - 1\n",
    "\tword_pos.reverse()\n",
    "\treturn word_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your maximal matching algorithm on a toy dictionary\n",
    "\n",
    "Expected output:\n",
    "\n",
    "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
    "<br>\n",
    "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, None, 4] !\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
      "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
      "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
      "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
      "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
      "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
      "[None, None, None, None, None, None, None, None, None, 4] !\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี!\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your backtracking algorithm on a toy dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "ไป|หา|มเหสี|!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(2, 3)\n",
      "(4, 8)\n",
      "(9, 9)\n",
      "ไป|หา|มเหสี|!\n"
     ]
    }
   ],
   "source": [
    "def print_tokenized_text(d, input_text):\n",
    "    tokenized_text=[]\n",
    "    for pos in backtrack(d):\n",
    "        print(pos)\n",
    "        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n",
    "\n",
    "    print(\"|\".join(tokenized_text))\n",
    "    \n",
    "print_tokenized_text(out,input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try it on a real dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-01-13 06:58:12--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1560612 (1.5M) [text/plain]\n",
      "Saving to: 'words_th.txt.3'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3%  123K 12s\n",
      "    50K .......... .......... .......... .......... ..........  6%  293K 8s\n",
      "   100K .......... .......... .......... .......... ..........  9%  273K 7s\n",
      "   150K .......... .......... .......... .......... .......... 13% 1.31M 5s\n",
      "   200K .......... .......... .......... .......... .......... 16% 1.67M 4s\n",
      "   250K .......... .......... .......... .......... .......... 19%  392K 4s\n",
      "   300K .......... .......... .......... .......... .......... 22% 2.26M 3s\n",
      "   350K .......... .......... .......... .......... .......... 26% 1.44M 3s\n",
      "   400K .......... .......... .......... .......... .......... 29% 1.78M 2s\n",
      "   450K .......... .......... .......... .......... .......... 32% 1.38M 2s\n",
      "   500K .......... .......... .......... .......... .......... 36%  685K 2s\n",
      "   550K .......... .......... .......... .......... .......... 39% 1.26M 2s\n",
      "   600K .......... .......... .......... .......... .......... 42%  873K 2s\n",
      "   650K .......... .......... .......... .......... .......... 45% 1.40M 2s\n",
      "   700K .......... .......... .......... .......... .......... 49% 1.40M 1s\n",
      "   750K .......... .......... .......... .......... .......... 52% 1.34M 1s\n",
      "   800K .......... .......... .......... .......... .......... 55% 2.20M 1s\n",
      "   850K .......... .......... .......... .......... .......... 59%  732K 1s\n",
      "   900K .......... .......... .......... .......... .......... 62% 1.38M 1s\n",
      "   950K .......... .......... .......... .......... .......... 65% 1.19M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 68% 1.83M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 72% 1.70M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 75%  943K 1s\n",
      "  1150K .......... .......... .......... .......... .......... 78%  741K 0s\n",
      "  1200K .......... .......... .......... .......... .......... 82% 2.23M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 85% 1.22M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 88% 1.65M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 91% 1.43M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 95%  501K 0s\n",
      "  1450K .......... .......... .......... .......... .......... 98% 1.66M 0s\n",
      "  1500K .......... .......... ....                            100% 1.34M=2.0s\n",
      "\n",
      "2019-01-13 06:58:15 (777 KB/s) - 'words_th.txt.3' saved [1560612/1560612]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 63529\n",
      "['ก.', 'ก.ค.', 'ก.ต.', 'ก.ป.ส.', 'ก.พ.', 'ก.พ.ด.', 'ก.ม.', 'ก.ย', 'ก.ย.', 'ก.ร.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n",
    "    thai_vocab = f.read().splitlines() \n",
    "print(\"Vocab size:\", len(thai_vocab))\n",
    "print(thai_vocab[:10])\n",
    "#you can add more vocab to the dictionary \n",
    "thai_vocab.extend([\"ๆ\",\"!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output of your maximal matching algoithm on a new dictionary\n",
    "Expected output:\n",
    "<br>\n",
    "[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
    "<br>\n",
    "[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n",
    "<br>\n",
    "[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, inf, inf, inf, inf, inf, inf] า\n",
    "<br>\n",
    "[None, None, None, None, 2, inf, inf, inf, 2] ม\n",
    "<br>\n",
    "[None, None, None, None, None, inf, 3, inf, inf] เ\n",
    "<br>\n",
    "[None, None, None, None, None, None, inf, inf, inf] ห\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, 4, 4] ส\n",
    "<br>\n",
    "[None, None, None, None, None, None, None, None, None] ี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
      "[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n",
      "[None, None, None, 3, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 2, inf, inf, inf, 2] ม\n",
      "[None, None, None, None, None, 3, 3, inf, inf] เ\n",
      "[None, None, None, None, None, None, 4, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4] ส\n",
      "[None, None, None, None, None, None, None, None, 5] ี\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i],input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected tokenized text\n",
    "ไปหา|มเหสี"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "(4, 8)\n",
      "ไปหา|มเหสี\n"
     ]
    }
   ],
   "source": [
    "print_tokenized_text(out,input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
